{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MagicSepCNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoltenMuffins/MagicSchoolBus/blob/master/MagicSepCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szu07hn92OS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Thank you https://developers.google.com/machine-learning/guides/text-classification/step-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAuegDlwaNyq",
        "colab_type": "text"
      },
      "source": [
        "## 2. Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjAQe_gY3EeY",
        "colab_type": "code",
        "outputId": "82ebee4c-94ef-4823-bc9f-d4a8eec1c00b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#Download the files from the Kaggle NSDC page\n",
        "!wget -qq https://www.dropbox.com/s/dlpf8lirpf69nvb/ndsc-beginner.zip\n",
        "\n",
        "#Unzip the zip file\n",
        "!unzip -qq -n ndsc-beginner.zip\n",
        "\n",
        "#Remove zipfile, we won't need it anymore\n",
        "!rm ndsc-beginner.zip\n",
        "\n",
        "#Check directory contents\n",
        "!ls\n",
        "\n",
        "#The files we want are all in the folder ndsc-beginner"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " google_ndsc_sepcnn_wbatchnorm.csv   sample_data\n",
            " keras-adabound\t\t\t     tzuchieh_sepcnn_batchnorm\n",
            " __MACOSX\t\t\t    'yuning_sepcnn(1)_wbatchnorm_model.h5'\n",
            " ndsc-beginner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgsb6c-UbDKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgHJiUvP4q76",
        "colab_type": "code",
        "outputId": "34d5cb9a-565f-4bfa-a2f4-273e3979aedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "source": [
        "train_df = pd.read_csv(\"./ndsc-beginner/train.csv\")\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemid</th>\n",
              "      <th>title</th>\n",
              "      <th>Category</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>307504</td>\n",
              "      <td>nyx sex bomb pallete natural palette</td>\n",
              "      <td>0</td>\n",
              "      <td>beauty_image/6b2e9cbb279ac95703348368aa65da09.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>461203</td>\n",
              "      <td>etude house precious mineral any cushion pearl...</td>\n",
              "      <td>1</td>\n",
              "      <td>beauty_image/20450222d857c9571ba8fa23bdedc8c9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3592295</td>\n",
              "      <td>milani rose powder blush</td>\n",
              "      <td>2</td>\n",
              "      <td>beauty_image/6a5962bed605a3dd6604ca3a4278a4f9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4460167</td>\n",
              "      <td>etude house baby sweet sugar powder</td>\n",
              "      <td>3</td>\n",
              "      <td>beauty_image/56987ae186e8a8e71fcc5a261ca485da.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5853995</td>\n",
              "      <td>bedak revlon color stay aqua mineral make up</td>\n",
              "      <td>3</td>\n",
              "      <td>beauty_image/9c6968066ebab57588c2f757a240d8b9.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    itemid                                              title  Category  \\\n",
              "0   307504               nyx sex bomb pallete natural palette         0   \n",
              "1   461203  etude house precious mineral any cushion pearl...         1   \n",
              "2  3592295                           milani rose powder blush         2   \n",
              "3  4460167                etude house baby sweet sugar powder         3   \n",
              "4  5853995       bedak revlon color stay aqua mineral make up         3   \n",
              "\n",
              "                                          image_path  \n",
              "0  beauty_image/6b2e9cbb279ac95703348368aa65da09.jpg  \n",
              "1  beauty_image/20450222d857c9571ba8fa23bdedc8c9.jpg  \n",
              "2  beauty_image/6a5962bed605a3dd6604ca3a4278a4f9.jpg  \n",
              "3  beauty_image/56987ae186e8a8e71fcc5a261ca485da.jpg  \n",
              "4  beauty_image/9c6968066ebab57588c2f757a240d8b9.jpg  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnZAbXHIymU1",
        "colab_type": "code",
        "outputId": "e1e70c9c-18d9-415b-aafd-5906ee7e6043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(666615, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqmK7hVRaaqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "train_texts = train_df['title'].tolist()\n",
        "train_labels = np.array(train_df['Category'].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1DcgiFc5mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Returns the median number of words per sample given corpus.\n",
        "\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)\n",
        "\n",
        "def plot_sample_length_distribution(sample_texts):\n",
        "    \"\"\"Plots the sample length distribution.\n",
        "\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "    \"\"\"\n",
        "    plt.hist([len(s) for s in sample_texts], 50)\n",
        "    plt.xlabel('Length of a sample')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Sample length distribution')\n",
        "    plt.show()\n",
        "    \n",
        "def get_max_words_all_sample(sample_texts):\n",
        "    \"\"\"Returns the median number of words per sample given corpus.\n",
        "\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.max(num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVmLr_cQc73s",
        "colab_type": "code",
        "outputId": "df216a47-1d19-4549-ed5f-d748385331ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "get_num_words_per_sample(train_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWlakbLRdkIA",
        "colab_type": "code",
        "outputId": "bf8a8ca3-f900-4b77-8a46-730ab6bea621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "get_max_words_all_sample(train_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U11cvL6Ix81K",
        "colab_type": "code",
        "outputId": "13c4486b-8555-493c-a766-1bcfb6a1a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "666615/9"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74068.33333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMAFKb5dxu8",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Choose Model\n",
        "\n",
        "1. Calculate the number of samples/number of words per sample ratio.\n",
        "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
        "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
        "flowchart below):\n",
        "  * Split the samples into word n-grams; convert the n-grams into vectors.\n",
        "  * Score the importance of the vectors and then select the top 20K using the scores.\n",
        "  * Build an MLP model.\n",
        "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
        "   sepCNN model to classify them (right branch in the flowchart below):   \n",
        "   * Split the samples into words; select the top 20K words based on their frequency.\n",
        "   * Convert the samples into word sequence vectors.\n",
        "   * If the original number of samples/number of words per sample ratio is less than 15K, using a fine-tuned pre-trained embedding with the sepCNN model will likely provide the best results.\n",
        "4. Measure the model performance with different hyperparameter values to find\n",
        "   the best model configuration for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx4a_KoYeUNJ",
        "colab_type": "code",
        "outputId": "6b685493-8151-48a3-992c-82148dd1b0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#Calculate samples divided (by number of words per sample)\n",
        "ratio = len(train_texts)/get_num_words_per_sample(train_texts)\n",
        "\n",
        "if ratio < 1500:\n",
        "  print('Use an MLP model')\n",
        "elif ratio > 1500:\n",
        "  print('Use sepCNN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use sepCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jkug6-ffrn",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://developers.google.com/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png =550x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZKLnowJrTEd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_xzIlX5f9Y_",
        "colab_type": "text"
      },
      "source": [
        "For the rest of this notebook we shall use the instructions contained in (Option B) to build our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrFEGCZgGS0",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare \n",
        "\n",
        "Before our data can be fed to a model, it needs to be transformed to a format the model can understand.\n",
        "\n",
        "First, the data samples that we have gathered may be in a specific order. We do not want any information associated with the ordering of samples to influence the relationship between texts and labels. For example, if a dataset is sorted by class and is then split into training/validation sets, these sets will not be representative of the overall distribution of data.\n",
        "\n",
        "A simple best practice to ensure the model is not affected by data order is to always shuffle the data before doing anything else. If your data is already split into training and validation sets, make sure to transform your validation data the same way you transform your training data. If you don’t already have separate training and validation sets, you can split the samples after shuffling; it’s typical to use 80% of the samples for training and 20% for validation.\n",
        "\n",
        "Second, machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
        "\n",
        "**Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
        "\n",
        "**Vectorization:** Define a good numerical measure to characterize these texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvZ_ab7hgIP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate Sequence Vectors\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "\n",
        "# Vectorization parameters\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Limit on the length of text sequences. Sequences longer than this\n",
        "# will be truncated.\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "max_length = 30\n",
        "\n",
        "def train_tokenizer(train_texts):\n",
        "    # Create vocabulary with training texts.\n",
        "    global tokenizer\n",
        "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "def sequence_vectorize(train_texts, val_texts, max_length):\n",
        "    \"\"\"Vectorizes texts as sequence vectors.\n",
        "\n",
        "    1 text = 1 sequence vector with fixed length.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        val_texts: list, validation text strings.\n",
        "\n",
        "    # Returns\n",
        "        x_train, x_val, word_index: vectorized training and validation\n",
        "            texts and word index dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorize training and validation texts.\n",
        "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "    # Get max sequence length.\n",
        "#     max_length = len(max(x_train, key=len))\n",
        "#     if max_length > MAX_SEQUENCE_LENGTH:\n",
        "#         max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    # Fix sequence length to max value. Sequences shorter than the length are\n",
        "    # padded in the beginning and sequences longer are truncated\n",
        "    # at the beginning.\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "    return x_train, x_val, tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLSV6D8hF9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasetlen = len(train_texts)\n",
        "train_df = train_df.sample(frac=1) #Shuffle rows\n",
        "\n",
        "a_df = train_df[:int(datasetlen*0.8)]\n",
        "b_df = train_df[int(datasetlen*0.2):]\n",
        "\n",
        "train_texts = a_df['title'].tolist()\n",
        "val_texts = b_df['title'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJKivsDhGTt",
        "colab_type": "text"
      },
      "source": [
        "**Label vectorization**\n",
        "\n",
        "We saw how to convert sample text data into numerical vectors. A similar process must be applied to the labels. We can simply convert labels into values in range [0, num_classes - 1]. For example, if there are 3 classes we can just use values 0, 1 and 2 to represent them. Internally, the network will use one-hot vectors to represent these values (to avoid inferring an incorrect relationship between labels). This representation depends on the loss function and the last- layer activation function we use in our neural network. We will learn more about these in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-fzwC95gsII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#No need to do this since our data is already in the form of numerical vectors\n",
        "train_labels = np.array(a_df['Category'].tolist())\n",
        "val_labels = np.array(b_df['Category'].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OKAUF7kjUut",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build, Train, and Evaluate Your Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXXFGzLCjbtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    \"\"\"Gets the # units and activation function for the last network layer.\n",
        "\n",
        "    # Arguments\n",
        "        num_classes: int, number of classes.\n",
        "\n",
        "    # Returns\n",
        "        units, activation values.\n",
        "    \"\"\"\n",
        "    if num_classes == 2:\n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    return units, activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzIpZsZnFvrl",
        "colab_type": "text"
      },
      "source": [
        "**LOAD_GLOVE_EMBEDDINGS** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3IihCoFrxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):\n",
        "    \"\"\"\n",
        "    Loads pre-trained word embeddings (GloVe embeddings)\n",
        "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
        "                - embedding_dim: dimension of each vector embedding\n",
        "                - generate_matrix: whether to generate an embedding matrix\n",
        "        Outputs:\n",
        "                - word2coefs: Dictionary. Word to its corresponding coefficients\n",
        "                - word2index: Dictionary. Word to word-index\n",
        "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
        "    \"\"\"\n",
        "    # First, build the \"word2coefs\" and \"word2index\"\n",
        "    word2coefs = {} # word to its corresponding coefficients\n",
        "    word2index = {} # word to word-index\n",
        "    with open(fp) as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            try:\n",
        "                data = [x.strip().lower() for x in line.split()]\n",
        "                word = data[0]\n",
        "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
        "                word2coefs[word] = coefs\n",
        "                if word not in word2index:\n",
        "                    word2index[word] = len(word2index)\n",
        "            except Exception as e:\n",
        "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
        "                continue\n",
        "        # End of for loop.\n",
        "    # End of with open\n",
        "    if include_empty_char:\n",
        "        word2index[''] = len(word2index)\n",
        "    # Second, build the \"embedding_matrix\"\n",
        "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
        "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, idx in word2index.items():\n",
        "        embedding_vec = word2coefs.get(word)\n",
        "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
        "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
        "    # return word2coefs, word2index, embedding_matrix\n",
        "    return word2index, np.asarray(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSgxwCcJjgJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The following code constructs a four-layer sepCNN model:\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.python.keras.layers import Activation\n",
        "from tensorflow import keras\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/gdrive')\n",
        "\n",
        "# import glove embeddings\n",
        "\n",
        "#word2index, embedding_matrix = load_glove_embeddings('/gdrive/My Drive/Copy of NDSC_Free_100d.txt', embedding_dim=100)\n",
        "# End import\n",
        "\n",
        "\n",
        "# from tensorflow.python.keras.layers import Bidirectional, CuDNNGRU\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                            output_dim=embedding_matrix.shape[1],\n",
        "                            input_length=max_length,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=False))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "    if blocks >= 1:\n",
        "      for _ in range(blocks-1):\n",
        "         # Model depth changes with var blocks\n",
        "          model.add(Dropout(rate=dropout_rate))\n",
        "          model.add(SeparableConv1D(filters=filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    activation='relu',\n",
        "                                    bias_initializer='random_uniform',\n",
        "                                    depthwise_initializer='random_uniform',\n",
        "                                    padding='same'))\n",
        "          model.add(SeparableConv1D(filters=filters,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    activation='relu',\n",
        "                                    bias_initializer='random_uniform',\n",
        "                                    depthwise_initializer='random_uniform',\n",
        "                                    padding='same'))\n",
        "          model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "      model.add(SeparableConv1D(filters=filters * 2,\n",
        "                                kernel_size=kernel_size,\n",
        "  #                               activation='relu',\n",
        "                                bias_initializer='random_uniform',\n",
        "                                depthwise_initializer='random_uniform',\n",
        "                                kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                                padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(SeparableConv1D(filters=filters * 2,\n",
        "                                kernel_size=kernel_size,\n",
        "  #                               activation='relu',\n",
        "                                bias_initializer='random_uniform',\n",
        "                                depthwise_initializer='random_uniform',\n",
        "                                kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                                padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(GlobalAveragePooling1D())\n",
        "      model.add(Dropout(rate=dropout_rate))\n",
        "      model.add(Dense(op_units, activation=op_activation))\n",
        "    \n",
        "    # Enable blocks = 0.5\n",
        "    elif blocks == 0.5:\n",
        "      model.add(SeparableConv1D(filters=filters * 2,\n",
        "                                kernel_size=kernel_size,\n",
        "  #                               activation='relu',\n",
        "                                bias_initializer='random_uniform',\n",
        "                                depthwise_initializer='random_uniform',\n",
        "                                kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                                padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(GlobalAveragePooling1D())\n",
        "      model.add(Dropout(rate=dropout_rate))\n",
        "      model.add(Dense(op_units, activation=op_activation))\n",
        "      \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnpuRRTmwW2",
        "colab_type": "text"
      },
      "source": [
        "## 4.5 Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7EqfxyPMeOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def _load_and_shuffle_data(data_path,\n",
        "                           file_name,\n",
        "                           seed):\n",
        "    \"\"\"Loads and shuffles the dataset using pandas.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        file_name: string, name of the data file.\n",
        "        cols: list, columns to load from the data file.\n",
        "        seed: int, seed for randomizer.\n",
        "        separator: string, separator to use for splitting data.\n",
        "        header: int, row to use as data header.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    data_path = os.path.join(data_path, file_name)\n",
        "    data = pd.read_csv(data_path)\n",
        "    return data.reindex(np.random.permutation(data.index))\n",
        "\n",
        "def _split_training_and_validation_sets(texts, labels, validation_split):\n",
        "    \"\"\"Splits the texts and labels into training and validation sets.\n",
        "    # Arguments\n",
        "        texts: list, text data.\n",
        "        labels: list, label data.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "    \"\"\"\n",
        "    num_training_samples = int((1 - validation_split) * len(texts))\n",
        "    return ((texts[:num_training_samples], labels[:num_training_samples]),\n",
        "            (texts[num_training_samples:], labels[num_training_samples:]))  \n",
        "  \n",
        "# Replaces load_data and loads our title data instead.\n",
        "\n",
        "def load_nsdc_dataset(data_path, validation_split=0.2, seed=123):\n",
        "    \"\"\"Loads the shopee nsdc basic classification challenge dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 124848\n",
        "        Number of test samples: 31212\n",
        "        Number of categories: 5 (0 - negative, 1 - somewhat negative,\n",
        "                2 - neutral, 3 - somewhat positive, 4 - positive)\n",
        "    \"\"\"\n",
        "    columns = (1, 2)  # 2 - Phrases, 3 - Sentiment.\n",
        "    data = _load_and_shuffle_data(data_path, 'train.csv', seed)\n",
        "\n",
        "    # Get the review phrase and sentiment values.\n",
        "    texts = list(data['title'])\n",
        "    labels = np.array(data['Category'])\n",
        "    return _split_training_and_validation_sets(texts, labels, validation_split)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLqbAriBYB4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = './ndsc-beginner/'\n",
        "data = load_nsdc_dataset(data_path, validation_split=0.2, seed=123)\n",
        "test_df = pd.read_csv(\"./ndsc-beginner/test.csv\") \n",
        "test_texts = list(test_df['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xm4ysJ4XCcy",
        "colab_type": "text"
      },
      "source": [
        "# Adjust Hyperparameter Values Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uz2IW2iWzcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limit on the number of words to considered for word vectorization\n",
        "TOP_K = 10000\n",
        "\n",
        "#Hyperparams\n",
        "learning_rate=5e-1\n",
        "#This is the minimum learn rate value allowed despite ReduceLearningRateonPlateau\n",
        "min_lr = 0.0000001\n",
        "lr_factor = 0.2\n",
        "patience = 5\n",
        "\n",
        "epochs=40\n",
        "batch_size=64\n",
        "blocks=1\n",
        "filters=128\n",
        "dropout_rate=0.3\n",
        "embedding_dim=100\n",
        "kernel_size=9\n",
        "pool_size=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDub2XoPk7Qr",
        "colab_type": "code",
        "outputId": "178a4efa-113f-4434-c006-71d02b0f904a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\"\"\"\n",
        "Vectorizes training and validation texts into sequences and uses that for\n",
        "training a sequence model - a sepCNN model. We use sequence model for text\n",
        "classification when the ratio of number of samples to number of words per\n",
        "sample for the given dataset is very large (>~15K).\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# import build_model\n",
        "# import load_data\n",
        "# import vectorize_data\n",
        "# import explore_data\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\"\"\"Trains sequence model on the given dataset.\n",
        "# Arguments\n",
        "    data: tuples of training and test texts and labels.\n",
        "    learning_rate: float, learning rate for training model.\n",
        "    epochs: int, number of epochs.\n",
        "    batch_size: int, number of samples per batch.\n",
        "    blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "    filters: int, output dimension of sepCNN layers in the model.\n",
        "    dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "    embedding_dim: int, dimension of the embedding vectors.\n",
        "    kernel_size: int, length of the convolution window.\n",
        "    pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "# Raises\n",
        "    ValueError: If validation data has label values which were not seen\n",
        "        in the training data.\n",
        "\"\"\"\n",
        "# Get the data.\n",
        "(train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "# Verify that validation labels are in the same range as training labels.\n",
        "# num_classes = explore_data.get_num_classes(train_labels)\n",
        "num_classes = 58\n",
        "unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "if len(unexpected_labels):\n",
        "    raise ValueError('Unexpected label values found in the validation set:'\n",
        "                     ' {unexpected_labels}. Please make sure that the '\n",
        "                     'labels in the validation set are in the same range '\n",
        "                     'as training labels.'.format(\n",
        "                         unexpected_labels=unexpected_labels))\n",
        "\n",
        "# Vectorize texts.\n",
        "#   x_train, x_val, word_index = vectorize_data.sequence_vectorize(\n",
        "#           train_texts, val_texts)\n",
        "all_texts = train_texts + test_texts\n",
        "print(all_texts[0:9])\n",
        "train_tokenizer(all_texts)\n",
        "x_train, x_val, word_index = sequence_vectorize(train_texts, val_texts, 30)\n",
        "\n",
        "# Number of features will be the embedding input dimension. Add 1 for the\n",
        "# reserved index 0.\n",
        "num_features = min(len(word_index) + 1, TOP_K)\n",
        "print('num_features:{}'.format(num_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['osella off shoulder stripe brand matahari', 'wanita mini dress segar print o neck lengan pendek kasual longgar gadis summer', '5 g natural mica powder', 'ysl le cushion engraved name', 'make over camuflage cream face concealer', 'terbatas ultima ii wonderwear stay last liquid make up foundation', 'bestseller wet n and wild photofocus photo focus foundation peach natural', 'bedak ertos baked powder all in 1', 'makeup base liquid foundation longlasting waterproof moisturizer oil control']\n",
            "num_features:10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c4RNNMGDOSI",
        "colab_type": "code",
        "outputId": "5b091a93-2e6d-4020-c780-6f2162c123f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!git clone https://github.com/titu1994/keras-adabound.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-adabound' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QokP_946fhlI",
        "colab_type": "code",
        "outputId": "f2eb1281-c4d5-4214-84b6-3ed131bb47a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "model = sepcnn_model(blocks=blocks,\n",
        "                     filters=filters,\n",
        "                     kernel_size=kernel_size,\n",
        "                     embedding_dim=embedding_dim,\n",
        "                     dropout_rate=dropout_rate,\n",
        "                     pool_size=pool_size,\n",
        "                     input_shape=x_train.shape[1:],\n",
        "                     num_classes=num_classes,\n",
        "                     num_features=num_features)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss = 'sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 100)           1000000   \n",
            "_________________________________________________________________\n",
            "separable_conv1d (SeparableC (None, 30, 256)           26756     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1 (Batc (None, 30, 256)           1024      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "separable_conv1d_1 (Separabl (None, 30, 256)           68096     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_1 (Ba (None, 30, 256)           1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 58)                14906     \n",
            "=================================================================\n",
            "Total params: 1,111,806\n",
            "Trainable params: 1,110,782\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKDyy3qv5Dbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "#Callbacks ring ring\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=lr_factor,\n",
        "                              patience=patience,\n",
        "                              verbose = 1,\n",
        "                              min_lr=min_lr)\n",
        "\n",
        "checkpoint = ModelCheckpoint('sepcnn_batchnorm', \n",
        "                             monitor='val_acc', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=False, \n",
        "                             mode='auto', \n",
        "                             period=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlQMcZaAfkpn",
        "colab_type": "code",
        "outputId": "4ee23b60-e65c-497c-b440-8b02ba7aeb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2512
        }
      },
      "source": [
        "# Train and validate model.\n",
        "history = model.fit(\n",
        "        x_train,\n",
        "        train_labels,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, val_labels),\n",
        "        verbose=1,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[reduce_lr, checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 533292 samples, validate on 133323 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6629\n",
            "Epoch 00001: val_acc improved from -inf to 0.69788, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 98s 183us/sample - loss: 1.1312 - acc: 0.6629 - val_loss: 0.9721 - val_acc: 0.6979\n",
            "Epoch 2/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.9642 - acc: 0.7009\n",
            "Epoch 00002: val_acc improved from 0.69788 to 0.70862, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.9641 - acc: 0.7009 - val_loss: 0.9252 - val_acc: 0.7086\n",
            "Epoch 3/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.9215 - acc: 0.7099\n",
            "Epoch 00003: val_acc improved from 0.70862 to 0.71090, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.9215 - acc: 0.7099 - val_loss: 0.9136 - val_acc: 0.7109\n",
            "Epoch 4/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.8935 - acc: 0.7164\n",
            "Epoch 00004: val_acc improved from 0.71090 to 0.71245, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.8934 - acc: 0.7164 - val_loss: 0.9098 - val_acc: 0.7125\n",
            "Epoch 5/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.8722 - acc: 0.7215\n",
            "Epoch 00005: val_acc improved from 0.71245 to 0.71679, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 181us/sample - loss: 0.8722 - acc: 0.7215 - val_loss: 0.9013 - val_acc: 0.7168\n",
            "Epoch 6/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.8538 - acc: 0.7265\n",
            "Epoch 00006: val_acc improved from 0.71679 to 0.71936, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.8538 - acc: 0.7265 - val_loss: 0.8867 - val_acc: 0.7194\n",
            "Epoch 7/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7303\n",
            "Epoch 00007: val_acc improved from 0.71936 to 0.72390, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 95s 178us/sample - loss: 0.8379 - acc: 0.7303 - val_loss: 0.8728 - val_acc: 0.7239\n",
            "Epoch 8/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.8249 - acc: 0.7332\n",
            "Epoch 00008: val_acc did not improve from 0.72390\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.8249 - acc: 0.7332 - val_loss: 0.8656 - val_acc: 0.7237\n",
            "Epoch 9/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.7361\n",
            "Epoch 00009: val_acc improved from 0.72390 to 0.72652, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.8122 - acc: 0.7361 - val_loss: 0.8693 - val_acc: 0.7265\n",
            "Epoch 10/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7396\n",
            "Epoch 00010: val_acc did not improve from 0.72652\n",
            "533292/533292 [==============================] - 96s 179us/sample - loss: 0.8005 - acc: 0.7396 - val_loss: 0.8625 - val_acc: 0.7262\n",
            "Epoch 11/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7429\n",
            "Epoch 00011: val_acc did not improve from 0.72652\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.7883 - acc: 0.7429 - val_loss: 0.8611 - val_acc: 0.7250\n",
            "Epoch 12/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.7783 - acc: 0.7449\n",
            "Epoch 00012: val_acc did not improve from 0.72652\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.7783 - acc: 0.7450 - val_loss: 0.8676 - val_acc: 0.7260\n",
            "Epoch 13/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.7686 - acc: 0.7480\n",
            "Epoch 00013: val_acc improved from 0.72652 to 0.73034, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 98s 184us/sample - loss: 0.7687 - acc: 0.7480 - val_loss: 0.8617 - val_acc: 0.7303\n",
            "Epoch 14/40\n",
            "533120/533292 [============================>.] - ETA: 0s - loss: 0.7603 - acc: 0.7499\n",
            "Epoch 00014: val_acc did not improve from 0.73034\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.7602 - acc: 0.7500 - val_loss: 0.8658 - val_acc: 0.7291\n",
            "Epoch 15/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7530\n",
            "Epoch 00015: val_acc did not improve from 0.73034\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.7496 - acc: 0.7530 - val_loss: 0.8599 - val_acc: 0.7295\n",
            "Epoch 16/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7553\n",
            "Epoch 00016: val_acc did not improve from 0.73034\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.7407 - acc: 0.7553 - val_loss: 0.8640 - val_acc: 0.7299\n",
            "Epoch 17/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.7324 - acc: 0.7578\n",
            "Epoch 00017: val_acc did not improve from 0.73034\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.7324 - acc: 0.7578 - val_loss: 0.8763 - val_acc: 0.7291\n",
            "Epoch 18/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.7243 - acc: 0.7598\n",
            "Epoch 00018: val_acc improved from 0.73034 to 0.73047, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.7243 - acc: 0.7598 - val_loss: 0.8811 - val_acc: 0.7305\n",
            "Epoch 19/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.7158 - acc: 0.7619\n",
            "Epoch 00019: val_acc did not improve from 0.73047\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.7157 - acc: 0.7620 - val_loss: 0.8866 - val_acc: 0.7296\n",
            "Epoch 20/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.7644\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.1.\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.73047\n",
            "533292/533292 [==============================] - 103s 193us/sample - loss: 0.7076 - acc: 0.7644 - val_loss: 0.8671 - val_acc: 0.7305\n",
            "Epoch 21/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.6499 - acc: 0.7809\n",
            "Epoch 00021: val_acc improved from 0.73047 to 0.73717, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 109s 204us/sample - loss: 0.6498 - acc: 0.7809 - val_loss: 0.8727 - val_acc: 0.7372\n",
            "Epoch 22/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.7842\n",
            "Epoch 00022: val_acc improved from 0.73717 to 0.73756, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.6374 - acc: 0.7842 - val_loss: 0.8858 - val_acc: 0.7376\n",
            "Epoch 23/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.7860\n",
            "Epoch 00023: val_acc did not improve from 0.73756\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.6304 - acc: 0.7860 - val_loss: 0.9007 - val_acc: 0.7374\n",
            "Epoch 24/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.7879\n",
            "Epoch 00024: val_acc did not improve from 0.73756\n",
            "533292/533292 [==============================] - 97s 181us/sample - loss: 0.6253 - acc: 0.7879 - val_loss: 0.8993 - val_acc: 0.7372\n",
            "Epoch 25/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.7890\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.73756\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.6210 - acc: 0.7890 - val_loss: 0.8979 - val_acc: 0.7370\n",
            "Epoch 26/40\n",
            "533248/533292 [============================>.] - ETA: 0s - loss: 0.6009 - acc: 0.7953\n",
            "Epoch 00026: val_acc improved from 0.73756 to 0.73800, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.6009 - acc: 0.7953 - val_loss: 0.8985 - val_acc: 0.7380\n",
            "Epoch 27/40\n",
            "533120/533292 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.7958\n",
            "Epoch 00027: val_acc improved from 0.73800 to 0.73817, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 97s 182us/sample - loss: 0.5980 - acc: 0.7958 - val_loss: 0.8978 - val_acc: 0.7382\n",
            "Epoch 28/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.7967\n",
            "Epoch 00028: val_acc did not improve from 0.73817\n",
            "533292/533292 [==============================] - 96s 180us/sample - loss: 0.5961 - acc: 0.7967 - val_loss: 0.9061 - val_acc: 0.7381\n",
            "Epoch 29/40\n",
            "533056/533292 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.7968\n",
            "Epoch 00029: val_acc did not improve from 0.73817\n",
            "533292/533292 [==============================] - 104s 195us/sample - loss: 0.5937 - acc: 0.7968 - val_loss: 0.9135 - val_acc: 0.7378\n",
            "Epoch 30/40\n",
            "533120/533292 [============================>.] - ETA: 0s - loss: 0.5926 - acc: 0.7970\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.73817\n",
            "533292/533292 [==============================] - 109s 205us/sample - loss: 0.5926 - acc: 0.7970 - val_loss: 0.9126 - val_acc: 0.7380\n",
            "Epoch 31/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.5875 - acc: 0.7992\n",
            "Epoch 00031: val_acc did not improve from 0.73817\n",
            "533292/533292 [==============================] - 99s 185us/sample - loss: 0.5875 - acc: 0.7992 - val_loss: 0.9119 - val_acc: 0.7379\n",
            "Epoch 32/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.5869 - acc: 0.7989\n",
            "Epoch 00032: val_acc did not improve from 0.73817\n",
            "533292/533292 [==============================] - 99s 186us/sample - loss: 0.5870 - acc: 0.7989 - val_loss: 0.9124 - val_acc: 0.7378\n",
            "Epoch 33/40\n",
            "533184/533292 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.7995\n",
            "Epoch 00033: val_acc improved from 0.73817 to 0.73842, saving model to tzuchieh_sepcnn_batchnorm\n",
            "533292/533292 [==============================] - 96s 179us/sample - loss: 0.5862 - acc: 0.7995 - val_loss: 0.9112 - val_acc: 0.7384\n",
            "Epoch 34/40\n",
            "532992/533292 [============================>.] - ETA: 0s - loss: 0.5863 - acc: 0.7992\n",
            "Epoch 00034: val_acc did not improve from 0.73842\n",
            "533292/533292 [==============================] - 99s 186us/sample - loss: 0.5863 - acc: 0.7992 - val_loss: 0.9136 - val_acc: 0.7380\n",
            "Epoch 35/40\n",
            "388672/533292 [====================>.........] - ETA: 27s - loss: 0.5850 - acc: 0.8004Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fyJggXmiGG9",
        "colab_type": "text"
      },
      "source": [
        "After running the model for some time, look through the epochs and identify the epoch with the best validation_accuracy. Copy and paste the epoch line into the google sheet. Thanks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6nlUEnmfyEC",
        "colab_type": "code",
        "outputId": "d899ff70-5ba8-4c16-b48b-4ec53cf3d1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Print results.\n",
        "history = history.history\n",
        "print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.7381697297096252, loss: 0.9169268518741954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggI3Yho4qqAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If the model is good, save the model to file, then download it!\n",
        "model.save('sepcnn_wbatchnorm_model.h5')\n",
        "# return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CWfoItDCVh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# classifier = LogisticRegression()\n",
        "# classifier.fit(x_train, train_labels)\n",
        "# score = classifier.score(x_val, val_labels)\n",
        "\n",
        "# print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8fJfZmptp3Y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65kl5MwutqMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K4Lf30GgGXr",
        "colab_type": "text"
      },
      "source": [
        "## Make Inferences\n",
        "\n",
        "With the model fit, we can now use it to make inferences on our actual test data which we do not have labels for.\n",
        "\n",
        "We then save the predicted classes for each title in csv format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7A5YjHXvUet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We can ignore x_trash\n",
        "x_test, x_trash, word_index = sequence_vectorize(test_df['title'], ['hi']*len(test_df['title']), 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0jc1UX4RUVq",
        "colab_type": "code",
        "outputId": "7babb37b-7bae-48a3-91b3-6720e2fc641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "print(x_test[1:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0  129  344  314  287  147  371   29    3\n",
            "    61  627]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0   26  237 1711   77  432  619   29    3 2356\n",
            "   122 1285]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMUmIteKiVqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = model.predict(x_test)\n",
        "yeet = [np.argmax(pred) for pred in test_preds]\n",
        "\n",
        "submit_df = pd.DataFrame({\"itemid\": test_df[\"itemid\"], \"Category\": yeet})\n",
        "submit_df.to_csv(\"google_ndsc_sepcnn_wbatchnorm.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIIYLFHGix5J",
        "colab_type": "code",
        "outputId": "43297f23-171a-45d2-aec2-2e557ef61179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "print(submit_df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Category      itemid\n",
            "0         5   370855998\n",
            "1         5   637234604\n",
            "2         5   690282890\n",
            "3         5   930913462\n",
            "4         5  1039280071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WypWDqnClCmB",
        "colab_type": "text"
      },
      "source": [
        "## Look at Predictions/Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiOIf80slGc7",
        "colab_type": "code",
        "outputId": "4cfb86c9-4260-4850-f723-7db7d945352d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "from collections import Counter\n",
        "x = list(range(0, 58))\n",
        "print(x)\n",
        "\n",
        "#Spaghetti Line (aglio olio flavored)\n",
        "ClassifierFrequency = [x[1] for x in np.array(sorted(Counter(yeet).items()))]\n",
        "#Haxx cause our classifier didn't predict a class 57\n",
        "# ClassifierFrequency.append(-1)\n",
        "print(ClassifierFrequency)\n",
        "\n",
        "#Also plot frequency of each cat meow (this is bolognese)\n",
        "Ground_Truth_Freq = [x[1] for x in sorted(Counter(train_labels.tolist()).items())]\n",
        "\n",
        "print(Ground_Truth_Freq)\n",
        "\n",
        "#Plot em\n",
        "plt.plot(x,ClassifierFrequency,Ground_Truth_Freq)\n",
        "#Add Legend\n",
        "plt.gca().legend(('ClassifierFrequency','Ground_Truth_Freq'))\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "#Show em\n",
        "plt.show()\n",
        "\n",
        "#Save a this plot for future reference and comparison\n",
        "#Right Click > Save Image As...\n",
        "#Change filename to biLSTMbal01"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]\n",
            "[210, 598, 300, 1919, 1840, 2193, 11, 304, 87, 152, 15, 28, 55208, 6431, 3759, 683, 2790, 189, 2692, 128, 321, 118, 213, 13, 97, 19051, 20276, 6963, 3161, 2020, 209, 7173, 7857, 1289, 3753, 5985, 277, 614, 1332, 195, 109, 5147, 2966, 1747, 254, 540, 210, 230, 134, 130, 69, 137, 23, 97, 79, 35, 29, 12]\n",
            "[3017, 22880, 9252, 64782, 34211, 44221, 1619, 9304, 4884, 6446, 851, 3296, 17335, 2399, 2155, 482, 1771, 2206, 45211, 10834, 15889, 8414, 12269, 1330, 3404, 27298, 27234, 12949, 5200, 2673, 1148, 22174, 23800, 3879, 11731, 24512, 827, 1812, 3784, 555, 263, 15541, 8393, 4937, 757, 1742, 549, 753, 326, 463, 227, 308, 100, 340, 248, 123, 145, 39]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFQCAYAAABu9Q2aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUSs2bTml2UN",
        "colab_type": "text"
      },
      "source": [
        "## 5: Tune Hyperparameters\n",
        "We had to choose a number of hyperparameters for defining and training the model. We relied on intuition, examples and best practice recommendations. Our first choice of hyperparameter values, however, may not yield the best results. It only gives us a good starting point for training. Every problem is different and tuning these hyperparameters will help refine our model to better represent the particularities of the problem at hand. Let’s take a look at some of the hyperparameters we used and what it means to tune them:\n",
        "\n",
        "### Number of layers in the model: \n",
        "The number of layers in a neural network is an indicator of its complexity. We must be careful in choosing this value. Too many layers will allow the model to learn too much information about the training data, causing overfitting. Too few layers can limit the model’s learning ability, causing underfitting. For text classification datasets, we experimented with one, two, and three-layer MLPs. Models with two layers performed well, and in some cases better than three-layer models. Similarly, we tried sepCNNs with four and six layers, and the four-layer models performed well.\n",
        "\n",
        "### Number of units per layer:\n",
        "The units in a layer must hold the information for the transformation that a layer performs. For the first layer, this is driven by the number of features. In subsequent layers, the number of units depends on the choice of expanding or contracting the representation from the previous layer. Try to minimize the information loss between layers. We tried unit values in the range [8, 16, 32, 64], and 32/64 units worked well.\n",
        "\n",
        "### Dropout rate:\n",
        "Dropout layers are used in the model for regularization. They define the fraction of input to drop as a precaution for overfitting. Recommended range: 0.2–0.5.\n",
        "\n",
        "### Learning rate:\n",
        "This is the rate at which the neural network weights change between iterations. A large learning rate may cause large swings in the weights, and we may never find their optimal values. A low learning rate is good, but the model will take more iterations to converge. It is a good idea to start low, say at 1e-4. If the training is very slow, increase this value. If your model is not learning, try decreasing learning rate.\n",
        "\n",
        "There are couple of additional hyperparameters we tuned that are specific to our sepCNN model:\n",
        "\n",
        "### Kernel size:\n",
        "The size of the convolution window. Recommended values: 3 or 5.\n",
        "\n",
        "### Embedding dimensions:\n",
        "The number of dimensions we want to use to represent word embeddings—i.e., the size of each word vector. Recommended values: 50–300. In our experiments, we used GloVe embeddings with 200 dimensions with a pre- trained embedding layer.\n",
        "\n",
        "\n",
        "```\n",
        "Play around with these hyperparameters and see what works best. Once you have chosen the best-performing hyperparameters for your use case, your model is ready to be deployed.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmYMobSwnZ17",
        "colab_type": "text"
      },
      "source": [
        "Refer to [the tutorial](https://github.com/google/eng-edu/tree/master/ml/guides/text_classification) for instructions on how to build additional parts of the model"
      ]
    }
  ]
}
